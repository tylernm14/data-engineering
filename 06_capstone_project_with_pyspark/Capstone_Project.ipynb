{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "# Exploring Immigration Trends\n",
    "### Data Engineering Capstone Project\n",
    "\n",
    "#### Summary\n",
    "In this project we will be processing data to aid in the exploration of possible relationships between US immigration data, temperature data, and city demographic data. We will create an ETL pipeline to produce a dataset which can be loaded into a data warehouse and analyzed by data scientists.\n",
    "\n",
    "The project has the following steps:\n",
    "* Step 1: Scope the Project and Gather Data\n",
    "* Step 2: Explore and Assess the Data\n",
    "* Step 3: Define the Data Model\n",
    "* Step 4: Run ETL to Model the Data\n",
    "* Step 5: Complete Project Write Up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Do all imports and installs here\n",
    "import pandas as pd\n",
    "import re\n",
    "import os\n",
    "from datetime import datetime, timedelta\n",
    "from functools import reduce\n",
    "from pyspark.sql import SparkSession\n",
    "import pyspark.sql.functions as F\n",
    "from pyspark.sql.functions import udf, month, year, col, monotonically_increasing_id\n",
    "from pyspark.sql.types import BooleanType\n",
    "\n",
    "pd.set_option('display.max_columns', 200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.\\\n",
    "config(\"spark.jars.packages\",\"saurfang:spark-sas7bdat:2.0.0-s_2.11\")\\\n",
    ".enableHiveSupport().getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### Step 1: Scope the Project and Gather Data\n",
    "\n",
    "#### Scope \n",
    "We will be combining immigration data with temperature and demographic data by city to create a data model for exploring where foreigners travel based on temperature and destination city demographics.  This could be used to target ads at the demogrpahics arriving at specific cities during certain times of year.\n",
    "\n",
    "Some example queries would be \n",
    "\n",
    " - \"What city gets the most visitors in a particular month?\"\n",
    " - \"In which month does a particular city get the most tourists?\"\n",
    " - \"From where do most tourists visiting a specific city originate?\"\n",
    " - \"Top cities visited by foreigners from a specific country?\"\n",
    " - \"Difference in visitors during hottest and coldest month of a year\"\n",
    " - \"Average/median age of tourist visiting hot cities?\"\n",
    " - \"Average age of tourist visiting cold cities?\"\n",
    " - \"Top countries whose residents visit demographically youngest cities?\"\n",
    " - \"In what months are demographically older cities more popular with tourists?\"\n",
    "\n",
    "It is worth noting that the city of entry is not necessarily where the arriving person is visiting.  Tourists frequently travel to another destination within the US during their stay.  The immigration dataset tracks this with the `i94addr` field which denotes the state in which the respondent will reside during her stay.  However the temperature and city demographic datasets use a city field to denote location.  Therefore, for this analysis we are going to assume that most visitors are visiting the city in which they enter the USA and not travelling on to other destinations with significantly different weather.\n",
    "\n",
    "#### Describe and Gather Data \n",
    "\n",
    "##### **Immigration Dataset**: [Link](https://travel.trade.gov/research/reports/i94/historical/2016.html)\n",
    "We use 2016 I94 immigration data gathered by the US National Tourism and Trade Office to record entries into the US by foreigners via land, sea, or air.  This data has been provided in SAS7BDAT files as part of the project and a sample label description has been provided.  Some interesting features of the dataset include:\n",
    "\n",
    "- ``i94yr``: 4 digit year of entry\n",
    "- `i94mon`: numeric month of entry\n",
    "- `i94cit`: 3 digit code for country of origin\n",
    "- `i94port`: 3 digit code for port of entry\n",
    "- `arrdate`: arrival date in USA (SAS date numeric field, time delta since 1/1/1960)\n",
    "- `i94mode`: enumeration of entry type (1=air, 2=sea, 3=land, 9='not reported')\n",
    "- `i94addr`: state of address during stay in the US\n",
    "- `depdate`: date of departure from USA (SAS date numeric field, time delta since 1/1/1960)\n",
    "- `i94bir`: age of respondent\n",
    "- `i94visa`: enumeration representing reason for visit (1=business, 2=pleasure, 3=student)\n",
    "- `biryear`: 4 digit birth year of respondent\n",
    "- `gender`: gender of respondent\n",
    "\n",
    "**Note:** We can use the provided `I94_SAS_Labels_Description` to retrieve the city of entry from the provided `i94port` code.  The matching city is what will be used to combine the immigration data with temperature and city demographic data.  The `i94port` codes do not match the codes for airports listed in the file `airport-codes_csv.csv`, so we will establish correspondence between datasets using the city name.\n",
    "\n",
    "##### **Temperature Dataset**: [Link](https://www.kaggle.com/berkeleyearth/climate-change-earth-surface-temperature-data)\n",
    "We are using montlhy averaged global temperature data downloaded from Kaggle and produced by Berkeley Earth.  The dataset has records from 1843-2013.  Below are the columns of the dataset:\n",
    "\n",
    "- `dt`: date of measurement (YYYY-MM-DD)\n",
    "- `AverageTemperature`: average temperature recorded\n",
    "- `AverageTemperatureUncertainty`: average uncertainty of measurement\n",
    "- `City`: city of measurement\n",
    "- `Country`: country of measurement\n",
    "- `Latitude`: geographic latitude\n",
    "- `Longitude`: geographic longitude\n",
    "\n",
    "##### **US City Demographics Dataset**: [Link](https://public.opendatasoft.com/explore/dataset/us-cities-demographics/export/)\n",
    "The city demographic data comes from OpenSoft, licensed under the public domain.   It contains demographics data from all US cities and census-designated places with a population greater or equal to 65,000.  It was produced by US Census Bureau's 2015 American Community Survey.  Some interesting attributes for our purposes are:\n",
    "\n",
    "- `City`: city/municipality\n",
    "- `State`: US state in which the city belongs\n",
    "- `Median Age`: median age of resident (decimal)\n",
    "- `Male Population`: count of male residents\n",
    "- `Female Population`: count of female residents\n",
    "- `Total Population`: count of all residents\n",
    "- `Number of Veterans`: count of veteran residents\n",
    "- `Foreign-born`: count of foreign-born residents\n",
    "- `Average household size`: average number of residents per household\n",
    "- `State Code`: state abbreviation\n",
    "- `Race`: race identified by respondent\n",
    "- `Count`: number of respondents identifying as reported race\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### Step 2: Explore and Assess the Data\n",
    "#### Explore the Data \n",
    "Here we view some data and identify quality issues, like missing values, duplicate data, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cicid</th>\n",
       "      <th>i94yr</th>\n",
       "      <th>i94mon</th>\n",
       "      <th>i94cit</th>\n",
       "      <th>i94res</th>\n",
       "      <th>i94port</th>\n",
       "      <th>arrdate</th>\n",
       "      <th>i94mode</th>\n",
       "      <th>i94addr</th>\n",
       "      <th>depdate</th>\n",
       "      <th>i94bir</th>\n",
       "      <th>i94visa</th>\n",
       "      <th>count</th>\n",
       "      <th>dtadfile</th>\n",
       "      <th>visapost</th>\n",
       "      <th>occup</th>\n",
       "      <th>entdepa</th>\n",
       "      <th>entdepd</th>\n",
       "      <th>entdepu</th>\n",
       "      <th>matflag</th>\n",
       "      <th>biryear</th>\n",
       "      <th>dtaddto</th>\n",
       "      <th>gender</th>\n",
       "      <th>insnum</th>\n",
       "      <th>airline</th>\n",
       "      <th>admnum</th>\n",
       "      <th>fltno</th>\n",
       "      <th>visatype</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>6.0</td>\n",
       "      <td>2016.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>692.0</td>\n",
       "      <td>692.0</td>\n",
       "      <td>XXX</td>\n",
       "      <td>20573.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>37.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>T</td>\n",
       "      <td>NaN</td>\n",
       "      <td>U</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1979.0</td>\n",
       "      <td>10282016</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.897628e+09</td>\n",
       "      <td>NaN</td>\n",
       "      <td>B2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>7.0</td>\n",
       "      <td>2016.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>254.0</td>\n",
       "      <td>276.0</td>\n",
       "      <td>ATL</td>\n",
       "      <td>20551.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>AL</td>\n",
       "      <td>NaN</td>\n",
       "      <td>25.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>20130811</td>\n",
       "      <td>SEO</td>\n",
       "      <td>NaN</td>\n",
       "      <td>G</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Y</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1991.0</td>\n",
       "      <td>D/S</td>\n",
       "      <td>M</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3.736796e+09</td>\n",
       "      <td>00296</td>\n",
       "      <td>F1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>15.0</td>\n",
       "      <td>2016.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>101.0</td>\n",
       "      <td>101.0</td>\n",
       "      <td>WAS</td>\n",
       "      <td>20545.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>MI</td>\n",
       "      <td>20691.0</td>\n",
       "      <td>55.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>20160401</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>T</td>\n",
       "      <td>O</td>\n",
       "      <td>NaN</td>\n",
       "      <td>M</td>\n",
       "      <td>1961.0</td>\n",
       "      <td>09302016</td>\n",
       "      <td>M</td>\n",
       "      <td>NaN</td>\n",
       "      <td>OS</td>\n",
       "      <td>6.666432e+08</td>\n",
       "      <td>93</td>\n",
       "      <td>B2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>16.0</td>\n",
       "      <td>2016.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>101.0</td>\n",
       "      <td>101.0</td>\n",
       "      <td>NYC</td>\n",
       "      <td>20545.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>MA</td>\n",
       "      <td>20567.0</td>\n",
       "      <td>28.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>20160401</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>NaN</td>\n",
       "      <td>M</td>\n",
       "      <td>1988.0</td>\n",
       "      <td>09302016</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>AA</td>\n",
       "      <td>9.246846e+10</td>\n",
       "      <td>00199</td>\n",
       "      <td>B2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>17.0</td>\n",
       "      <td>2016.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>101.0</td>\n",
       "      <td>101.0</td>\n",
       "      <td>NYC</td>\n",
       "      <td>20545.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>MA</td>\n",
       "      <td>20567.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>20160401</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>NaN</td>\n",
       "      <td>M</td>\n",
       "      <td>2012.0</td>\n",
       "      <td>09302016</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>AA</td>\n",
       "      <td>9.246846e+10</td>\n",
       "      <td>00199</td>\n",
       "      <td>B2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   cicid   i94yr  i94mon  i94cit  i94res i94port  arrdate  i94mode i94addr  \\\n",
       "0    6.0  2016.0     4.0   692.0   692.0     XXX  20573.0      NaN     NaN   \n",
       "1    7.0  2016.0     4.0   254.0   276.0     ATL  20551.0      1.0      AL   \n",
       "2   15.0  2016.0     4.0   101.0   101.0     WAS  20545.0      1.0      MI   \n",
       "3   16.0  2016.0     4.0   101.0   101.0     NYC  20545.0      1.0      MA   \n",
       "4   17.0  2016.0     4.0   101.0   101.0     NYC  20545.0      1.0      MA   \n",
       "\n",
       "   depdate  i94bir  i94visa  count  dtadfile visapost occup entdepa entdepd  \\\n",
       "0      NaN    37.0      2.0    1.0       NaN      NaN   NaN       T     NaN   \n",
       "1      NaN    25.0      3.0    1.0  20130811      SEO   NaN       G     NaN   \n",
       "2  20691.0    55.0      2.0    1.0  20160401      NaN   NaN       T       O   \n",
       "3  20567.0    28.0      2.0    1.0  20160401      NaN   NaN       O       O   \n",
       "4  20567.0     4.0      2.0    1.0  20160401      NaN   NaN       O       O   \n",
       "\n",
       "  entdepu matflag  biryear   dtaddto gender insnum airline        admnum  \\\n",
       "0       U     NaN   1979.0  10282016    NaN    NaN     NaN  1.897628e+09   \n",
       "1       Y     NaN   1991.0       D/S      M    NaN     NaN  3.736796e+09   \n",
       "2     NaN       M   1961.0  09302016      M    NaN      OS  6.666432e+08   \n",
       "3     NaN       M   1988.0  09302016    NaN    NaN      AA  9.246846e+10   \n",
       "4     NaN       M   2012.0  09302016    NaN    NaN      AA  9.246846e+10   \n",
       "\n",
       "   fltno visatype  \n",
       "0    NaN       B2  \n",
       "1  00296       F1  \n",
       "2     93       B2  \n",
       "3  00199       B2  \n",
       "4  00199       B2  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# One month of immigration data\n",
    "fname = '../../data/18-83510-I94-Data-2016/i94_apr16_sub.sas7bdat'\n",
    "df_immigration = pd.read_sas(fname, 'sas7bdat', encoding=\"ISO-8859-1\")\n",
    "df_immigration.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>dt</th>\n",
       "      <th>AverageTemperature</th>\n",
       "      <th>AverageTemperatureUncertainty</th>\n",
       "      <th>City</th>\n",
       "      <th>Country</th>\n",
       "      <th>Latitude</th>\n",
       "      <th>Longitude</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1743-11-01</td>\n",
       "      <td>6.068</td>\n",
       "      <td>1.737</td>\n",
       "      <td>Århus</td>\n",
       "      <td>Denmark</td>\n",
       "      <td>57.05N</td>\n",
       "      <td>10.33E</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1743-12-01</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Århus</td>\n",
       "      <td>Denmark</td>\n",
       "      <td>57.05N</td>\n",
       "      <td>10.33E</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1744-01-01</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Århus</td>\n",
       "      <td>Denmark</td>\n",
       "      <td>57.05N</td>\n",
       "      <td>10.33E</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1744-02-01</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Århus</td>\n",
       "      <td>Denmark</td>\n",
       "      <td>57.05N</td>\n",
       "      <td>10.33E</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1744-03-01</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Århus</td>\n",
       "      <td>Denmark</td>\n",
       "      <td>57.05N</td>\n",
       "      <td>10.33E</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           dt  AverageTemperature  AverageTemperatureUncertainty   City  \\\n",
       "0  1743-11-01               6.068                          1.737  Århus   \n",
       "1  1743-12-01                 NaN                            NaN  Århus   \n",
       "2  1744-01-01                 NaN                            NaN  Århus   \n",
       "3  1744-02-01                 NaN                            NaN  Århus   \n",
       "4  1744-03-01                 NaN                            NaN  Århus   \n",
       "\n",
       "   Country Latitude Longitude  \n",
       "0  Denmark   57.05N    10.33E  \n",
       "1  Denmark   57.05N    10.33E  \n",
       "2  Denmark   57.05N    10.33E  \n",
       "3  Denmark   57.05N    10.33E  \n",
       "4  Denmark   57.05N    10.33E  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Read world temperature data\n",
    "fname = '../../data2/GlobalLandTemperaturesByCity.csv'\n",
    "df_temperatures = pd.read_csv(fname)\n",
    "df_temperatures.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "##### Find range of temperature measurements and check consistency of using first of month as day of measurement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Min date of measurement: 1743-11-01 00:00:00\n",
      "Max date of measurement: 2013-09-01 00:00:00\n",
      "Smallest day of month of measurement 1\n",
      "Largest day of month of measurement 1\n"
     ]
    }
   ],
   "source": [
    "# convert time strings to datetime objects to find oldest and most recent measusrements\n",
    "df_temperatures['dt'] = pd.to_datetime(df_temperatures['dt'])\n",
    "print(\"Min date of measurement:\", df_temperatures['dt'].min(axis=0))\n",
    "print(\"Max date of measurement:\", df_temperatures['dt'].max(axis=0))\n",
    "\n",
    "#check if all measurements on the first of the month\n",
    "print('Smallest day of month of measurement', df_temperatures['dt'].dt.day.min(axis=0))\n",
    "print('Largest day of month of measurement', df_temperatures['dt'].dt.day.max(axis=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>City</th>\n",
       "      <th>State</th>\n",
       "      <th>Median Age</th>\n",
       "      <th>Male Population</th>\n",
       "      <th>Female Population</th>\n",
       "      <th>Total Population</th>\n",
       "      <th>Number of Veterans</th>\n",
       "      <th>Foreign-born</th>\n",
       "      <th>Average Household Size</th>\n",
       "      <th>State Code</th>\n",
       "      <th>Race</th>\n",
       "      <th>Count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Silver Spring</td>\n",
       "      <td>Maryland</td>\n",
       "      <td>33.8</td>\n",
       "      <td>40601.0</td>\n",
       "      <td>41862.0</td>\n",
       "      <td>82463</td>\n",
       "      <td>1562.0</td>\n",
       "      <td>30908.0</td>\n",
       "      <td>2.60</td>\n",
       "      <td>MD</td>\n",
       "      <td>Hispanic or Latino</td>\n",
       "      <td>25924</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Quincy</td>\n",
       "      <td>Massachusetts</td>\n",
       "      <td>41.0</td>\n",
       "      <td>44129.0</td>\n",
       "      <td>49500.0</td>\n",
       "      <td>93629</td>\n",
       "      <td>4147.0</td>\n",
       "      <td>32935.0</td>\n",
       "      <td>2.39</td>\n",
       "      <td>MA</td>\n",
       "      <td>White</td>\n",
       "      <td>58723</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Hoover</td>\n",
       "      <td>Alabama</td>\n",
       "      <td>38.5</td>\n",
       "      <td>38040.0</td>\n",
       "      <td>46799.0</td>\n",
       "      <td>84839</td>\n",
       "      <td>4819.0</td>\n",
       "      <td>8229.0</td>\n",
       "      <td>2.58</td>\n",
       "      <td>AL</td>\n",
       "      <td>Asian</td>\n",
       "      <td>4759</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Rancho Cucamonga</td>\n",
       "      <td>California</td>\n",
       "      <td>34.5</td>\n",
       "      <td>88127.0</td>\n",
       "      <td>87105.0</td>\n",
       "      <td>175232</td>\n",
       "      <td>5821.0</td>\n",
       "      <td>33878.0</td>\n",
       "      <td>3.18</td>\n",
       "      <td>CA</td>\n",
       "      <td>Black or African-American</td>\n",
       "      <td>24437</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Newark</td>\n",
       "      <td>New Jersey</td>\n",
       "      <td>34.6</td>\n",
       "      <td>138040.0</td>\n",
       "      <td>143873.0</td>\n",
       "      <td>281913</td>\n",
       "      <td>5829.0</td>\n",
       "      <td>86253.0</td>\n",
       "      <td>2.73</td>\n",
       "      <td>NJ</td>\n",
       "      <td>White</td>\n",
       "      <td>76402</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               City          State  Median Age  Male Population  \\\n",
       "0     Silver Spring       Maryland        33.8          40601.0   \n",
       "1            Quincy  Massachusetts        41.0          44129.0   \n",
       "2            Hoover        Alabama        38.5          38040.0   \n",
       "3  Rancho Cucamonga     California        34.5          88127.0   \n",
       "4            Newark     New Jersey        34.6         138040.0   \n",
       "\n",
       "   Female Population  Total Population  Number of Veterans  Foreign-born  \\\n",
       "0            41862.0             82463              1562.0       30908.0   \n",
       "1            49500.0             93629              4147.0       32935.0   \n",
       "2            46799.0             84839              4819.0        8229.0   \n",
       "3            87105.0            175232              5821.0       33878.0   \n",
       "4           143873.0            281913              5829.0       86253.0   \n",
       "\n",
       "   Average Household Size State Code                       Race  Count  \n",
       "0                    2.60         MD         Hispanic or Latino  25924  \n",
       "1                    2.39         MA                      White  58723  \n",
       "2                    2.58         AL                      Asian   4759  \n",
       "3                    3.18         CA  Black or African-American  24437  \n",
       "4                    2.73         NJ                      White  76402  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fname = './us-cities-demographics.csv'\n",
    "df_city_demos = pd.read_csv(fname, delimiter=';')\n",
    "df_city_demos.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "##### Check rows for a specific city"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>City</th>\n",
       "      <th>State</th>\n",
       "      <th>Median Age</th>\n",
       "      <th>Male Population</th>\n",
       "      <th>Female Population</th>\n",
       "      <th>Total Population</th>\n",
       "      <th>Number of Veterans</th>\n",
       "      <th>Foreign-born</th>\n",
       "      <th>Average Household Size</th>\n",
       "      <th>State Code</th>\n",
       "      <th>Race</th>\n",
       "      <th>Count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>945</th>\n",
       "      <td>San Francisco</td>\n",
       "      <td>California</td>\n",
       "      <td>38.3</td>\n",
       "      <td>439752.0</td>\n",
       "      <td>425064.0</td>\n",
       "      <td>864816</td>\n",
       "      <td>26276.0</td>\n",
       "      <td>297199.0</td>\n",
       "      <td>2.37</td>\n",
       "      <td>CA</td>\n",
       "      <td>Hispanic or Latino</td>\n",
       "      <td>132114</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2569</th>\n",
       "      <td>San Francisco</td>\n",
       "      <td>California</td>\n",
       "      <td>38.3</td>\n",
       "      <td>439752.0</td>\n",
       "      <td>425064.0</td>\n",
       "      <td>864816</td>\n",
       "      <td>26276.0</td>\n",
       "      <td>297199.0</td>\n",
       "      <td>2.37</td>\n",
       "      <td>CA</td>\n",
       "      <td>Asian</td>\n",
       "      <td>324034</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2756</th>\n",
       "      <td>San Francisco</td>\n",
       "      <td>California</td>\n",
       "      <td>38.3</td>\n",
       "      <td>439752.0</td>\n",
       "      <td>425064.0</td>\n",
       "      <td>864816</td>\n",
       "      <td>26276.0</td>\n",
       "      <td>297199.0</td>\n",
       "      <td>2.37</td>\n",
       "      <td>CA</td>\n",
       "      <td>Black or African-American</td>\n",
       "      <td>53270</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2805</th>\n",
       "      <td>San Francisco</td>\n",
       "      <td>California</td>\n",
       "      <td>38.3</td>\n",
       "      <td>439752.0</td>\n",
       "      <td>425064.0</td>\n",
       "      <td>864816</td>\n",
       "      <td>26276.0</td>\n",
       "      <td>297199.0</td>\n",
       "      <td>2.37</td>\n",
       "      <td>CA</td>\n",
       "      <td>White</td>\n",
       "      <td>442155</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2838</th>\n",
       "      <td>San Francisco</td>\n",
       "      <td>California</td>\n",
       "      <td>38.3</td>\n",
       "      <td>439752.0</td>\n",
       "      <td>425064.0</td>\n",
       "      <td>864816</td>\n",
       "      <td>26276.0</td>\n",
       "      <td>297199.0</td>\n",
       "      <td>2.37</td>\n",
       "      <td>CA</td>\n",
       "      <td>American Indian and Alaska Native</td>\n",
       "      <td>8997</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               City       State  Median Age  Male Population  \\\n",
       "945   San Francisco  California        38.3         439752.0   \n",
       "2569  San Francisco  California        38.3         439752.0   \n",
       "2756  San Francisco  California        38.3         439752.0   \n",
       "2805  San Francisco  California        38.3         439752.0   \n",
       "2838  San Francisco  California        38.3         439752.0   \n",
       "\n",
       "      Female Population  Total Population  Number of Veterans  Foreign-born  \\\n",
       "945            425064.0            864816             26276.0      297199.0   \n",
       "2569           425064.0            864816             26276.0      297199.0   \n",
       "2756           425064.0            864816             26276.0      297199.0   \n",
       "2805           425064.0            864816             26276.0      297199.0   \n",
       "2838           425064.0            864816             26276.0      297199.0   \n",
       "\n",
       "      Average Household Size State Code                               Race  \\\n",
       "945                     2.37         CA                 Hispanic or Latino   \n",
       "2569                    2.37         CA                              Asian   \n",
       "2756                    2.37         CA          Black or African-American   \n",
       "2805                    2.37         CA                              White   \n",
       "2838                    2.37         CA  American Indian and Alaska Native   \n",
       "\n",
       "       Count  \n",
       "945   132114  \n",
       "2569  324034  \n",
       "2756   53270  \n",
       "2805  442155  \n",
       "2838    8997  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_city_demos[df_city_demos['City'] == 'San Francisco']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "**Note:\n",
    "As show above, the demographics data set is denormalized on race.  There is a count for every race in a city**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### Cleaning Steps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "We need to know the valid countires of origin and valid cities where people enter the US.  For this we create a lookup tables with valid values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "def create_codes_to_cities(fpath):\n",
    "    \"\"\"\n",
    "    Create a dict of valid i94 port codes to city\n",
    "    \n",
    "    Params:\n",
    "    fpath: filepath to file of valid port codes to cities\n",
    "    \"\"\"\n",
    "    codes_to_cities = {}\n",
    "    with open(fpath, 'r') as file:\n",
    "        for line in file:\n",
    "            match = re.search(r\"^\\s+\\'(.+)\\'\\s+=\\s+\\'(.+)\\'\\s+$\", line)\n",
    "            if match:\n",
    "                s = match[2].rstrip().split(', ')\n",
    "                if len(s) > 1:\n",
    "                    state = s[-1][0:2].lower()\n",
    "                    city = ','.join(s[0:-1]).lower()\n",
    "                else:\n",
    "                    s = s[0].rstrip().split(' ')\n",
    "                    state = s[-1].lower()\n",
    "                    city = ','.join(s[0:-1]).lower()\n",
    "                codes_to_cities[match[1]] = { 'city': city, 'state': state }\n",
    "    return codes_to_cities\n",
    "\n",
    "\n",
    "def create_cities_to_codes(codes_to_cities):\n",
    "    \"\"\"\n",
    "    Create a reverse index of the codes_to_cities dictionary\n",
    "    \n",
    "    Params:\n",
    "    codes_to_cities: a dictionary with with port codes to cities\n",
    "    \"\"\"\n",
    "    cities_to_codes = {}\n",
    "    for k, v in codes_to_cities.items():\n",
    "        cities_to_codes[v['city']] = k\n",
    "    return cities_to_codes\n",
    "    \n",
    "    \n",
    "codes_to_cities = create_codes_to_cities('./port_code_to_city.txt')\n",
    "cities_to_codes = create_cities_to_codes(codes_to_cities)\n",
    "\n",
    "@udf \n",
    "def city_to_i94port_code(city):\n",
    "    \"\"\" Get i94port code from city \"\"\"\n",
    "    return cities_to_codes.get(city.lower(), None)\n",
    "\n",
    "def create_codes_to_countries(fpath):\n",
    "    \"\"\"\n",
    "    Create a dict of i94 codes to country\n",
    "    \n",
    "    Params:\n",
    "    fpath: filepath to file of codes to countries\n",
    "    \"\"\"\n",
    "    codes_to_countries = {}\n",
    "    with open(fpath, 'r') as file:\n",
    "        for line in file:\n",
    "            match = re.search(r\"\\s+(\\d{3})\\s+=\\s+\\'(\\w+)\\'\", line)\n",
    "            if match:\n",
    "                code = match[1]\n",
    "                country = match[2].split('INVALID: ')[-1]\n",
    "                codes_to_countries[int(code)] = country.lower()\n",
    "    return codes_to_countries\n",
    "\n",
    "codes_to_countries = create_codes_to_countries('./country_codes.txt')\n",
    "\n",
    "@udf\n",
    "def i94cit_code_to_country(code):\n",
    "    \"\"\" convert code to country \"\"\"\n",
    "    country = codes_to_countries.get(int(code), None)\n",
    "    if country:\n",
    "        country = country.title()\n",
    "    return country"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "def non_null_df(df, required_cols):\n",
    "    \"\"\" Returns rows of dataframe where non-null values are in all required columns \"\"\"\n",
    "    return df.where(reduce(lambda x, y: x & y, (col(x).isNotNull() for x in required_cols)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# UDF for checking i94port code validity\n",
    "def valid_i94port(x):\n",
    "    \"\"\" check if i94port is a valid code \"\"\"\n",
    "    return x in codes_to_cities\n",
    "\n",
    "# UDF for checking i94cit code validity\n",
    "def valid_i94cit(x):\n",
    "    \"\"\" check if i94cit is a valid code \"\"\"\n",
    "    return x in codes_to_countries\n",
    "\n",
    "i94port_filter = udf(valid_i94port, returnType=BooleanType())\n",
    "i94cit_filter = udf(valid_i94cit, returnType=BooleanType())\n",
    "    \n",
    "def clean_immigration_data(df):\n",
    "    \"\"\"\n",
    "    Remove rows with NaNs in required columns \n",
    "    or invalid port or invalid origin country\n",
    "    \"\"\"\n",
    "    required_cols = ['i94yr', 'i94mon', 'i94cit', 'i94port', 'arrdate', 'i94mode',\n",
    "                     'depdate', 'i94addr', 'gender', 'i94bir', 'i94visa']\n",
    "    \n",
    "    df = df.filter(i94port_filter(df.i94port) & i94cit_filter(df.i94cit))\n",
    "    df = non_null_df(df, required_cols)\n",
    "    return df\n",
    "\n",
    "# Test\n",
    "# fpath = '../../data/18-83510-I94-Data-2016/i94_apr16_sub.sas7bdat'\n",
    "# df_imms = spark.read.format('com.github.saurfang.sas.spark').load(fpath)\n",
    "# df_imms = clean_immigration_data(df_imms)\n",
    "# df_imms.toPandas().head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "def clean_temperature_data(df, min_year=None, max_year=None):\n",
    "    \"\"\" \n",
    "    Remove rows with NaNs in required colsand filter for only US cities \n",
    "    Params:\n",
    "    min_year: lowest year of data\n",
    "    max_year: hightest year of data\n",
    "    \"\"\"\n",
    "    required_cols = ['dt', 'AverageTemperature', 'City', 'Country', 'Latitude', 'Longitude']\n",
    "    df = df.filter(col('Country') == 'United States')\n",
    "    if min_year:\n",
    "        df = df.filter(year('dt') >= min_year)\n",
    "    if max_year:\n",
    "        df = df.filter(year('dt') >= max_year)\n",
    "    df = non_null_df(df, required_cols)\n",
    "    return df\n",
    "\n",
    "# Test\n",
    "# fpath = '../../data2/GlobalLandTemperaturesByCity.csv'\n",
    "# df_temps = spark.read.format('csv').option('header', 'true').load(fpath)\n",
    "# df_temps = clean_temperature_data(df_temps)\n",
    "# df_temps.toPandas().head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>min(max(dt))</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2013-08-01</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  min(max(dt))\n",
       "0   2013-08-01"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Retrive the max measurement date for all cities\n",
    "df_max_measurement_date = (df_temps.withColumn('datetime', col('dt').cast('timestamp')).groupBy('City').agg(F.max('dt')))\n",
    "df_max_measurement_date.toPandas()\n",
    "\n",
    "# Retrive the min measurement\n",
    "df_max_measurement_date.agg(F.min('max(dt)')).toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### Test getting most recent month's temps for a specific city"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>City</th>\n",
       "      <th>month(dt)</th>\n",
       "      <th>max(dt)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>San Francisco</td>\n",
       "      <td>9</td>\n",
       "      <td>2013-09-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>San Francisco</td>\n",
       "      <td>11</td>\n",
       "      <td>2012-11-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>San Francisco</td>\n",
       "      <td>12</td>\n",
       "      <td>2012-12-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>San Francisco</td>\n",
       "      <td>8</td>\n",
       "      <td>2013-08-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>San Francisco</td>\n",
       "      <td>10</td>\n",
       "      <td>2012-10-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>San Francisco</td>\n",
       "      <td>2</td>\n",
       "      <td>2013-02-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>San Francisco</td>\n",
       "      <td>1</td>\n",
       "      <td>2013-01-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>San Francisco</td>\n",
       "      <td>4</td>\n",
       "      <td>2013-04-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>San Francisco</td>\n",
       "      <td>6</td>\n",
       "      <td>2013-06-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>San Francisco</td>\n",
       "      <td>7</td>\n",
       "      <td>2013-07-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>San Francisco</td>\n",
       "      <td>5</td>\n",
       "      <td>2013-05-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>San Francisco</td>\n",
       "      <td>3</td>\n",
       "      <td>2013-03-01</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             City  month(dt)     max(dt)\n",
       "0   San Francisco          9  2013-09-01\n",
       "1   San Francisco         11  2012-11-01\n",
       "2   San Francisco         12  2012-12-01\n",
       "3   San Francisco          8  2013-08-01\n",
       "4   San Francisco         10  2012-10-01\n",
       "5   San Francisco          2  2013-02-01\n",
       "6   San Francisco          1  2013-01-01\n",
       "7   San Francisco          4  2013-04-01\n",
       "8   San Francisco          6  2013-06-01\n",
       "9   San Francisco          7  2013-07-01\n",
       "10  San Francisco          5  2013-05-01\n",
       "11  San Francisco          3  2013-03-01"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test getting most recent months' measurements for San Francisco\n",
    "df_temps_max_per_month = df_temps.groupBy(['City', month('dt')]).agg(F.max('dt'))\\\n",
    "                         .where(col('City') == 'San Francisco')\n",
    "df_temps_max_per_month.toPandas()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>City</th>\n",
       "      <th>month(dt)</th>\n",
       "      <th>max(dt)</th>\n",
       "      <th>dt</th>\n",
       "      <th>AverageTemperature</th>\n",
       "      <th>AverageTemperatureUncertainty</th>\n",
       "      <th>Country</th>\n",
       "      <th>Latitude</th>\n",
       "      <th>Longitude</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>San Francisco</td>\n",
       "      <td>1</td>\n",
       "      <td>2013-01-01</td>\n",
       "      <td>2013-01-01</td>\n",
       "      <td>8.32</td>\n",
       "      <td>0.278</td>\n",
       "      <td>United States</td>\n",
       "      <td>37.78N</td>\n",
       "      <td>122.03W</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>San Francisco</td>\n",
       "      <td>2</td>\n",
       "      <td>2013-02-01</td>\n",
       "      <td>2013-02-01</td>\n",
       "      <td>10.229</td>\n",
       "      <td>0.494</td>\n",
       "      <td>United States</td>\n",
       "      <td>37.78N</td>\n",
       "      <td>122.03W</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>San Francisco</td>\n",
       "      <td>3</td>\n",
       "      <td>2013-03-01</td>\n",
       "      <td>2013-03-01</td>\n",
       "      <td>13.505999999999998</td>\n",
       "      <td>0.325</td>\n",
       "      <td>United States</td>\n",
       "      <td>37.78N</td>\n",
       "      <td>122.03W</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>San Francisco</td>\n",
       "      <td>4</td>\n",
       "      <td>2013-04-01</td>\n",
       "      <td>2013-04-01</td>\n",
       "      <td>15.995999999999999</td>\n",
       "      <td>0.419</td>\n",
       "      <td>United States</td>\n",
       "      <td>37.78N</td>\n",
       "      <td>122.03W</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>San Francisco</td>\n",
       "      <td>5</td>\n",
       "      <td>2013-05-01</td>\n",
       "      <td>2013-05-01</td>\n",
       "      <td>17.434</td>\n",
       "      <td>0.327</td>\n",
       "      <td>United States</td>\n",
       "      <td>37.78N</td>\n",
       "      <td>122.03W</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>San Francisco</td>\n",
       "      <td>6</td>\n",
       "      <td>2013-06-01</td>\n",
       "      <td>2013-06-01</td>\n",
       "      <td>19.759</td>\n",
       "      <td>0.33799999999999997</td>\n",
       "      <td>United States</td>\n",
       "      <td>37.78N</td>\n",
       "      <td>122.03W</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>San Francisco</td>\n",
       "      <td>7</td>\n",
       "      <td>2013-07-01</td>\n",
       "      <td>2013-07-01</td>\n",
       "      <td>20.656999999999996</td>\n",
       "      <td>0.36</td>\n",
       "      <td>United States</td>\n",
       "      <td>37.78N</td>\n",
       "      <td>122.03W</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>San Francisco</td>\n",
       "      <td>8</td>\n",
       "      <td>2013-08-01</td>\n",
       "      <td>2013-08-01</td>\n",
       "      <td>19.730999999999998</td>\n",
       "      <td>0.522</td>\n",
       "      <td>United States</td>\n",
       "      <td>37.78N</td>\n",
       "      <td>122.03W</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>San Francisco</td>\n",
       "      <td>9</td>\n",
       "      <td>2013-09-01</td>\n",
       "      <td>2013-09-01</td>\n",
       "      <td>20.471</td>\n",
       "      <td>0.826</td>\n",
       "      <td>United States</td>\n",
       "      <td>37.78N</td>\n",
       "      <td>122.03W</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>San Francisco</td>\n",
       "      <td>10</td>\n",
       "      <td>2012-10-01</td>\n",
       "      <td>2012-10-01</td>\n",
       "      <td>17.294999999999998</td>\n",
       "      <td>0.278</td>\n",
       "      <td>United States</td>\n",
       "      <td>37.78N</td>\n",
       "      <td>122.03W</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>San Francisco</td>\n",
       "      <td>11</td>\n",
       "      <td>2012-11-01</td>\n",
       "      <td>2012-11-01</td>\n",
       "      <td>13.367</td>\n",
       "      <td>0.23800000000000002</td>\n",
       "      <td>United States</td>\n",
       "      <td>37.78N</td>\n",
       "      <td>122.03W</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>San Francisco</td>\n",
       "      <td>12</td>\n",
       "      <td>2012-12-01</td>\n",
       "      <td>2012-12-01</td>\n",
       "      <td>8.95</td>\n",
       "      <td>0.37</td>\n",
       "      <td>United States</td>\n",
       "      <td>37.78N</td>\n",
       "      <td>122.03W</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             City  month(dt)     max(dt)          dt  AverageTemperature  \\\n",
       "0   San Francisco          1  2013-01-01  2013-01-01                8.32   \n",
       "1   San Francisco          2  2013-02-01  2013-02-01              10.229   \n",
       "2   San Francisco          3  2013-03-01  2013-03-01  13.505999999999998   \n",
       "3   San Francisco          4  2013-04-01  2013-04-01  15.995999999999999   \n",
       "4   San Francisco          5  2013-05-01  2013-05-01              17.434   \n",
       "5   San Francisco          6  2013-06-01  2013-06-01              19.759   \n",
       "6   San Francisco          7  2013-07-01  2013-07-01  20.656999999999996   \n",
       "7   San Francisco          8  2013-08-01  2013-08-01  19.730999999999998   \n",
       "8   San Francisco          9  2013-09-01  2013-09-01              20.471   \n",
       "9   San Francisco         10  2012-10-01  2012-10-01  17.294999999999998   \n",
       "10  San Francisco         11  2012-11-01  2012-11-01              13.367   \n",
       "11  San Francisco         12  2012-12-01  2012-12-01                8.95   \n",
       "\n",
       "   AverageTemperatureUncertainty        Country Latitude Longitude  \n",
       "0                          0.278  United States   37.78N   122.03W  \n",
       "1                          0.494  United States   37.78N   122.03W  \n",
       "2                          0.325  United States   37.78N   122.03W  \n",
       "3                          0.419  United States   37.78N   122.03W  \n",
       "4                          0.327  United States   37.78N   122.03W  \n",
       "5            0.33799999999999997  United States   37.78N   122.03W  \n",
       "6                           0.36  United States   37.78N   122.03W  \n",
       "7                          0.522  United States   37.78N   122.03W  \n",
       "8                          0.826  United States   37.78N   122.03W  \n",
       "9                          0.278  United States   37.78N   122.03W  \n",
       "10           0.23800000000000002  United States   37.78N   122.03W  \n",
       "11                          0.37  United States   37.78N   122.03W  "
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ordered_df_temp = df_temps_max_per_month.alias('df1').join(df_temps.alias('df2'),\n",
    "                                                           ((col('df1.max(dt)') == col('df2.dt'))\n",
    "                                                           & (col('df1.City') == col('df2.City'))),\n",
    "                                                           'inner') \\\n",
    "                                                     .drop(col('df2.City')) \\\n",
    "                                                     .orderBy(col('df1.City'), col('df1.month(dt)')).toPandas()\n",
    "ordered_df_temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "def clean_city_demo_data(df):\n",
    "    \"\"\" Remove rows with missing reqired data \"\"\"\n",
    "    required_cols = ['City', 'State', 'Median Age', 'Male Population',\n",
    "                     'Female Population', 'Total Population', 'Number of Veterans',\n",
    "                     'Foreign-born', 'Average Household Size', 'State Code', 'Race', 'Count']\n",
    "    df = non_null_df(df, required_cols)\n",
    "    return df\n",
    "\n",
    "# Test\n",
    "# fpath = './us-cities-demographics.csv'\n",
    "# df_city_demos = spark.read.format('csv').option('header', 'true').option('delimiter', ';').load(fpath)\n",
    "# df_city_demos = clean_city_demo_data(df_city_demos)\n",
    "# df_city_demos.toPandas().head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>City</th>\n",
       "      <th>State</th>\n",
       "      <th>Median Age</th>\n",
       "      <th>Male Population</th>\n",
       "      <th>Female Population</th>\n",
       "      <th>Total Population</th>\n",
       "      <th>Number of Veterans</th>\n",
       "      <th>Foreign-born</th>\n",
       "      <th>Average Household Size</th>\n",
       "      <th>State Code</th>\n",
       "      <th>Race</th>\n",
       "      <th>Count</th>\n",
       "      <th>i94port_code</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Silver Spring</td>\n",
       "      <td>Maryland</td>\n",
       "      <td>33.8</td>\n",
       "      <td>40601</td>\n",
       "      <td>41862</td>\n",
       "      <td>82463</td>\n",
       "      <td>1562</td>\n",
       "      <td>30908</td>\n",
       "      <td>2.6</td>\n",
       "      <td>MD</td>\n",
       "      <td>Hispanic or Latino</td>\n",
       "      <td>25924</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Quincy</td>\n",
       "      <td>Massachusetts</td>\n",
       "      <td>41.0</td>\n",
       "      <td>44129</td>\n",
       "      <td>49500</td>\n",
       "      <td>93629</td>\n",
       "      <td>4147</td>\n",
       "      <td>32935</td>\n",
       "      <td>2.39</td>\n",
       "      <td>MA</td>\n",
       "      <td>White</td>\n",
       "      <td>58723</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Hoover</td>\n",
       "      <td>Alabama</td>\n",
       "      <td>38.5</td>\n",
       "      <td>38040</td>\n",
       "      <td>46799</td>\n",
       "      <td>84839</td>\n",
       "      <td>4819</td>\n",
       "      <td>8229</td>\n",
       "      <td>2.58</td>\n",
       "      <td>AL</td>\n",
       "      <td>Asian</td>\n",
       "      <td>4759</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Rancho Cucamonga</td>\n",
       "      <td>California</td>\n",
       "      <td>34.5</td>\n",
       "      <td>88127</td>\n",
       "      <td>87105</td>\n",
       "      <td>175232</td>\n",
       "      <td>5821</td>\n",
       "      <td>33878</td>\n",
       "      <td>3.18</td>\n",
       "      <td>CA</td>\n",
       "      <td>Black or African-American</td>\n",
       "      <td>24437</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Newark</td>\n",
       "      <td>New Jersey</td>\n",
       "      <td>34.6</td>\n",
       "      <td>138040</td>\n",
       "      <td>143873</td>\n",
       "      <td>281913</td>\n",
       "      <td>5829</td>\n",
       "      <td>86253</td>\n",
       "      <td>2.73</td>\n",
       "      <td>NJ</td>\n",
       "      <td>White</td>\n",
       "      <td>76402</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               City          State Median Age Male Population  \\\n",
       "0     Silver Spring       Maryland       33.8           40601   \n",
       "1            Quincy  Massachusetts       41.0           44129   \n",
       "2            Hoover        Alabama       38.5           38040   \n",
       "3  Rancho Cucamonga     California       34.5           88127   \n",
       "4            Newark     New Jersey       34.6          138040   \n",
       "\n",
       "  Female Population Total Population Number of Veterans Foreign-born  \\\n",
       "0             41862            82463               1562        30908   \n",
       "1             49500            93629               4147        32935   \n",
       "2             46799            84839               4819         8229   \n",
       "3             87105           175232               5821        33878   \n",
       "4            143873           281913               5829        86253   \n",
       "\n",
       "  Average Household Size State Code                       Race  Count  \\\n",
       "0                    2.6         MD         Hispanic or Latino  25924   \n",
       "1                   2.39         MA                      White  58723   \n",
       "2                   2.58         AL                      Asian   4759   \n",
       "3                   3.18         CA  Black or African-American  24437   \n",
       "4                   2.73         NJ                      White  76402   \n",
       "\n",
       "  i94port_code  \n",
       "0         None  \n",
       "1         None  \n",
       "2         None  \n",
       "3         None  \n",
       "4         None  "
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_city_demos.withColumn('i94port_code', city_to_i94port_code(col('City'))).toPandas().head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### Step 3: Define the Data Model\n",
    "#### 3.1 Conceptual Data Model\n",
    "We will be putting the data into a star schema.  The fact table will be immigration events into the US, combined with the temperature at the port of entry and some city demographic data.  We will have three dimension tables, one for immigration, one for tempearture, and one for city demographic data.  Most of our queries will revolve around temperature, age, gender trends, etc. so we are including dataset attributes that are related to these concepts.\n",
    "\n",
    "Our data model assumes that the fact table and immigration table will appended over time and that the dimension tables for temperature and city demographics will be rebuilt when necessary to incorporate new data. This is important to remember as we will be putting the `i94port_code` into the rows of the temperature and city demographic dataframes to support joins between the fact and dimension tables.  We can expect temperature data to be relatively stable over time just like city demographic data.  If a new port is added to the valid port code list these dimension tables will need to be rebuilt, but this is expected to occur very infrequently.\n",
    "\n",
    "##### Immigration fact table: \n",
    "Our fact table consists of immigration entry data plus the average monthly temperature for the month of entry.  Additionally we will include the median age and total city population data in the fact table in order to make aggregation quieries faster as we expect those types of queries to be most common.\n",
    "\n",
    "##### Immigration dimension table:\n",
    "The immigration dimension table just includes the relevant immigration data for each person entering the US.\n",
    "\n",
    "##### Temperature dimension table:\n",
    "In the cleanup phase we parred the data to only US city data for the most recent 12 months recorded.  This data is denormalized as there are 12 rows per city in the dataset.  Rather than normalize the data we are going to add the i94port code to the temperature data to allow joins with the fact table.\n",
    "\n",
    "##### City Demographic dimension table:\n",
    "The city demographic data is denormalized with respect to race.  So instead of splicing and creating a 'city_race' race join table with the a 'count' attribute, we are going to add the i94port code to the demographic rows to indicate the port.  This will allow for joins between the fact table and this city demographic dimension table.\n",
    "\n",
    "#### 3.2 Mapping Out Data Pipelines\n",
    "Steps necessary to pipeline the data into the chosen data model\n",
    "1. Load the data into spark dataframes\n",
    "1. Clean the dataframes by ensuring there are no NaNs for required columns\n",
    "2. Scope the temperature data to contain only US cities and the most recent 12 months of measurement\n",
    "3. Create the dimension tables for immigration, temperature, and city demographics, converting dates/times to a standard format\n",
    "3. Drop cities from temperature and city demo dimension tables that dont have an i94port code\n",
    "4. Run data quality checks after creation of each dimension table\n",
    "4. Create the fact table for immigration event facts\n",
    "5. Run data quality checks after creation of the fact table\n",
    "5. Save the fact and dimension tables in parquet format for loading on HDFS compatible RDBMS like Redshift"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### Step 4: Run Pipelines to Model the Data \n",
    "#### 4.1 Create the data model\n",
    "Build the data pipelines to create the data model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "@udf\n",
    "def sas_daydelta_to_timestamp(sas_day_delta):\n",
    "    \"\"\" Convert SAS day delta time since 1960 to datetime string \"\"\"\n",
    "    d = datetime.strptime('1960-01-01', '%Y-%m-%d') + timedelta(days=sas_day_delta)\n",
    "    return d.strftime('%Y-%m-%d')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>year</th>\n",
       "      <th>month</th>\n",
       "      <th>origin_country_code</th>\n",
       "      <th>port_code</th>\n",
       "      <th>arr_date</th>\n",
       "      <th>dep_date</th>\n",
       "      <th>mode</th>\n",
       "      <th>addr_state_abbr</th>\n",
       "      <th>age</th>\n",
       "      <th>reason</th>\n",
       "      <th>birth_year</th>\n",
       "      <th>gender</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2016.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>101.0</td>\n",
       "      <td>WAS</td>\n",
       "      <td>2016-04-01</td>\n",
       "      <td>2016-08-25</td>\n",
       "      <td>1.0</td>\n",
       "      <td>MI</td>\n",
       "      <td>55.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1961.0</td>\n",
       "      <td>M</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2016.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>101.0</td>\n",
       "      <td>BOS</td>\n",
       "      <td>2016-04-01</td>\n",
       "      <td>2016-04-05</td>\n",
       "      <td>1.0</td>\n",
       "      <td>MA</td>\n",
       "      <td>58.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1958.0</td>\n",
       "      <td>M</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2016.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>101.0</td>\n",
       "      <td>ATL</td>\n",
       "      <td>2016-04-01</td>\n",
       "      <td>2016-04-05</td>\n",
       "      <td>1.0</td>\n",
       "      <td>MA</td>\n",
       "      <td>56.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1960.0</td>\n",
       "      <td>F</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2016.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>101.0</td>\n",
       "      <td>ATL</td>\n",
       "      <td>2016-04-01</td>\n",
       "      <td>2016-04-17</td>\n",
       "      <td>1.0</td>\n",
       "      <td>MA</td>\n",
       "      <td>62.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1954.0</td>\n",
       "      <td>M</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2016.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>101.0</td>\n",
       "      <td>ATL</td>\n",
       "      <td>2016-04-01</td>\n",
       "      <td>2016-05-04</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NJ</td>\n",
       "      <td>49.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1967.0</td>\n",
       "      <td>M</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2016.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>101.0</td>\n",
       "      <td>ATL</td>\n",
       "      <td>2016-04-01</td>\n",
       "      <td>2016-06-06</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NY</td>\n",
       "      <td>43.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1973.0</td>\n",
       "      <td>M</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>2016.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>101.0</td>\n",
       "      <td>HOU</td>\n",
       "      <td>2016-04-01</td>\n",
       "      <td>2016-04-10</td>\n",
       "      <td>1.0</td>\n",
       "      <td>TX</td>\n",
       "      <td>53.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1963.0</td>\n",
       "      <td>F</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>2016.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>101.0</td>\n",
       "      <td>NYC</td>\n",
       "      <td>2016-04-01</td>\n",
       "      <td>2016-04-17</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NJ</td>\n",
       "      <td>37.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1979.0</td>\n",
       "      <td>M</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>2016.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>101.0</td>\n",
       "      <td>NYC</td>\n",
       "      <td>2016-04-01</td>\n",
       "      <td>2016-04-23</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NJ</td>\n",
       "      <td>49.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1967.0</td>\n",
       "      <td>F</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>2016.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>101.0</td>\n",
       "      <td>NYC</td>\n",
       "      <td>2016-04-01</td>\n",
       "      <td>2016-05-01</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NY</td>\n",
       "      <td>33.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1983.0</td>\n",
       "      <td>M</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     year  month  origin_country_code port_code    arr_date    dep_date  mode  \\\n",
       "0  2016.0    4.0                101.0       WAS  2016-04-01  2016-08-25   1.0   \n",
       "1  2016.0    4.0                101.0       BOS  2016-04-01  2016-04-05   1.0   \n",
       "2  2016.0    4.0                101.0       ATL  2016-04-01  2016-04-05   1.0   \n",
       "3  2016.0    4.0                101.0       ATL  2016-04-01  2016-04-17   1.0   \n",
       "4  2016.0    4.0                101.0       ATL  2016-04-01  2016-05-04   1.0   \n",
       "5  2016.0    4.0                101.0       ATL  2016-04-01  2016-06-06   1.0   \n",
       "6  2016.0    4.0                101.0       HOU  2016-04-01  2016-04-10   1.0   \n",
       "7  2016.0    4.0                101.0       NYC  2016-04-01  2016-04-17   1.0   \n",
       "8  2016.0    4.0                101.0       NYC  2016-04-01  2016-04-23   1.0   \n",
       "9  2016.0    4.0                101.0       NYC  2016-04-01  2016-05-01   1.0   \n",
       "\n",
       "  addr_state_abbr   age  reason  birth_year gender  \n",
       "0              MI  55.0     2.0      1961.0      M  \n",
       "1              MA  58.0     1.0      1958.0      M  \n",
       "2              MA  56.0     1.0      1960.0      F  \n",
       "3              MA  62.0     2.0      1954.0      M  \n",
       "4              NJ  49.0     2.0      1967.0      M  \n",
       "5              NY  43.0     2.0      1973.0      M  \n",
       "6              TX  53.0     2.0      1963.0      F  \n",
       "7              NJ  37.0     2.0      1979.0      M  \n",
       "8              NJ  49.0     2.0      1967.0      F  \n",
       "9              NY  33.0     2.0      1983.0      M  "
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fpath = '../../data/18-83510-I94-Data-2016/i94_apr16_sub.sas7bdat'\n",
    "#fpath = '../../data/18-83510-I94-Data-2016/*.sas7bdat'\n",
    "df_imms = spark.read.format('com.github.saurfang.sas.spark').load(fpath)\n",
    "cleaned_imm_data = clean_immigration_data(df_imms)\n",
    "\n",
    "dim_imm = cleaned_imm_data.select(col('i94yr').alias('year'),\n",
    "                                  col('i94mon').alias('month'),\n",
    "                                  col('i94cit').alias('origin_country_code'),\n",
    "                                  col('i94port').alias('port_code'),\n",
    "                                  sas_daydelta_to_timestamp(col('arrdate')).alias('arr_date'),\n",
    "                                  sas_daydelta_to_timestamp(col('depdate')).alias('dep_date'),\n",
    "                                  col('i94mode').alias('mode'),\n",
    "                                  col('i94addr').alias('addr_state_abbr'),\n",
    "                                  col('i94bir').alias('age'),\n",
    "                                  col('i94visa').alias('reason'),\n",
    "                                  col('biryear').alias('birth_year'),\n",
    "                                  col('gender')\n",
    "                                 )\n",
    "dim_imm.toPandas().head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>avg_temperature</th>\n",
       "      <th>city</th>\n",
       "      <th>country</th>\n",
       "      <th>latitude</th>\n",
       "      <th>longitude</th>\n",
       "      <th>port_code</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2013-03-01</td>\n",
       "      <td>4.766</td>\n",
       "      <td>Seattle</td>\n",
       "      <td>United States</td>\n",
       "      <td>47.42N</td>\n",
       "      <td>121.97W</td>\n",
       "      <td>SEA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2013-05-01</td>\n",
       "      <td>19.159</td>\n",
       "      <td>Louisville</td>\n",
       "      <td>United States</td>\n",
       "      <td>37.78N</td>\n",
       "      <td>85.42W</td>\n",
       "      <td>LOU</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2012-11-01</td>\n",
       "      <td>13.367</td>\n",
       "      <td>Sacramento</td>\n",
       "      <td>United States</td>\n",
       "      <td>37.78N</td>\n",
       "      <td>122.03W</td>\n",
       "      <td>SAC</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2013-01-01</td>\n",
       "      <td>0.98</td>\n",
       "      <td>Tacoma</td>\n",
       "      <td>United States</td>\n",
       "      <td>47.42N</td>\n",
       "      <td>121.97W</td>\n",
       "      <td>TAC</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2013-02-01</td>\n",
       "      <td>10.229</td>\n",
       "      <td>San Jose</td>\n",
       "      <td>United States</td>\n",
       "      <td>37.78N</td>\n",
       "      <td>122.03W</td>\n",
       "      <td>SNJ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2013-05-01</td>\n",
       "      <td>20.62</td>\n",
       "      <td>Birmingham</td>\n",
       "      <td>United States</td>\n",
       "      <td>32.95N</td>\n",
       "      <td>87.13W</td>\n",
       "      <td>BHX</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>2013-05-01</td>\n",
       "      <td>24.09</td>\n",
       "      <td>Tampa</td>\n",
       "      <td>United States</td>\n",
       "      <td>28.13N</td>\n",
       "      <td>82.73W</td>\n",
       "      <td>TAM</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>2013-06-01</td>\n",
       "      <td>23.418000000000003</td>\n",
       "      <td>Albuquerque</td>\n",
       "      <td>United States</td>\n",
       "      <td>34.56N</td>\n",
       "      <td>107.03W</td>\n",
       "      <td>ABQ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>2013-07-01</td>\n",
       "      <td>25.678</td>\n",
       "      <td>Columbus</td>\n",
       "      <td>United States</td>\n",
       "      <td>32.95N</td>\n",
       "      <td>85.21W</td>\n",
       "      <td>CLM</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>2013-07-01</td>\n",
       "      <td>23.558000000000003</td>\n",
       "      <td>Columbus</td>\n",
       "      <td>United States</td>\n",
       "      <td>39.38N</td>\n",
       "      <td>83.24W</td>\n",
       "      <td>CLM</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         date     avg_temperature         city        country latitude  \\\n",
       "0  2013-03-01               4.766      Seattle  United States   47.42N   \n",
       "1  2013-05-01              19.159   Louisville  United States   37.78N   \n",
       "2  2012-11-01              13.367   Sacramento  United States   37.78N   \n",
       "3  2013-01-01                0.98       Tacoma  United States   47.42N   \n",
       "4  2013-02-01              10.229     San Jose  United States   37.78N   \n",
       "5  2013-05-01               20.62   Birmingham  United States   32.95N   \n",
       "6  2013-05-01               24.09        Tampa  United States   28.13N   \n",
       "7  2013-06-01  23.418000000000003  Albuquerque  United States   34.56N   \n",
       "8  2013-07-01              25.678     Columbus  United States   32.95N   \n",
       "9  2013-07-01  23.558000000000003     Columbus  United States   39.38N   \n",
       "\n",
       "  longitude port_code  \n",
       "0   121.97W       SEA  \n",
       "1    85.42W       LOU  \n",
       "2   122.03W       SAC  \n",
       "3   121.97W       TAC  \n",
       "4   122.03W       SNJ  \n",
       "5    87.13W       BHX  \n",
       "6    82.73W       TAM  \n",
       "7   107.03W       ABQ  \n",
       "8    85.21W       CLM  \n",
       "9    83.24W       CLM  "
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fpath = '../../data2/GlobalLandTemperaturesByCity.csv'\n",
    "df_temps = spark.read.format('csv').option('header', 'true').load(fpath)\n",
    "cleaned_temp_data = clean_temperature_data(df_temps)\n",
    "# get most recent measurement per month per city\n",
    "df_temps_max_per_month = cleaned_temp_data.groupBy(['City', month('dt')]).agg(F.max('dt'))\n",
    "ordered_temp_data = df_temps_max_per_month.alias('df1').join(cleaned_temp_data.alias('df2'),\n",
    "                                                             ((col('df1.max(dt)') == col('df2.dt'))\n",
    "                                                             & (col('df1.City') == col('df2.City'))),\n",
    "                                                             'inner') \\\n",
    "                                                       .drop(col('df2.City'))\n",
    "\n",
    "dim_temp = ordered_temp_data.select(col('dt').alias('date'),\n",
    "                                    col('AverageTemperature').alias('avg_temperature'),\n",
    "                                    col('City').alias('city'),\n",
    "                                    col('Country').alias('country'),\n",
    "                                    col('Latitude').alias('latitude'),\n",
    "                                    col('Longitude').alias('longitude'))\\\n",
    "                            .withColumn('port_code', city_to_i94port_code(col('city')))\\\n",
    "                            .dropna(subset=['port_code'])\n",
    "                                    \n",
    "dim_temp.toPandas().head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>city</th>\n",
       "      <th>state</th>\n",
       "      <th>median_age</th>\n",
       "      <th>male_population</th>\n",
       "      <th>female_population</th>\n",
       "      <th>total_population</th>\n",
       "      <th>veteran_population</th>\n",
       "      <th>foreign_born_population</th>\n",
       "      <th>avg_household_size</th>\n",
       "      <th>state_abbr</th>\n",
       "      <th>race</th>\n",
       "      <th>race_count</th>\n",
       "      <th>port_code</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Philadelphia</td>\n",
       "      <td>Pennsylvania</td>\n",
       "      <td>34.1</td>\n",
       "      <td>741270</td>\n",
       "      <td>826172</td>\n",
       "      <td>1567442</td>\n",
       "      <td>61995</td>\n",
       "      <td>205339</td>\n",
       "      <td>2.61</td>\n",
       "      <td>PA</td>\n",
       "      <td>Asian</td>\n",
       "      <td>122721</td>\n",
       "      <td>PHI</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Fort Myers</td>\n",
       "      <td>Florida</td>\n",
       "      <td>37.3</td>\n",
       "      <td>36850</td>\n",
       "      <td>37165</td>\n",
       "      <td>74015</td>\n",
       "      <td>4312</td>\n",
       "      <td>15365</td>\n",
       "      <td>2.45</td>\n",
       "      <td>FL</td>\n",
       "      <td>White</td>\n",
       "      <td>50169</td>\n",
       "      <td>FMY</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Laredo</td>\n",
       "      <td>Texas</td>\n",
       "      <td>28.8</td>\n",
       "      <td>124305</td>\n",
       "      <td>131484</td>\n",
       "      <td>255789</td>\n",
       "      <td>4921</td>\n",
       "      <td>68427</td>\n",
       "      <td>3.66</td>\n",
       "      <td>TX</td>\n",
       "      <td>American Indian and Alaska Native</td>\n",
       "      <td>1253</td>\n",
       "      <td>LAR</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>New Haven</td>\n",
       "      <td>Connecticut</td>\n",
       "      <td>29.9</td>\n",
       "      <td>63765</td>\n",
       "      <td>66545</td>\n",
       "      <td>130310</td>\n",
       "      <td>2567</td>\n",
       "      <td>25871</td>\n",
       "      <td>2.48</td>\n",
       "      <td>CT</td>\n",
       "      <td>American Indian and Alaska Native</td>\n",
       "      <td>2205</td>\n",
       "      <td>NWH</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Salt Lake City</td>\n",
       "      <td>Utah</td>\n",
       "      <td>32.1</td>\n",
       "      <td>98364</td>\n",
       "      <td>94296</td>\n",
       "      <td>192660</td>\n",
       "      <td>6829</td>\n",
       "      <td>32166</td>\n",
       "      <td>2.38</td>\n",
       "      <td>UT</td>\n",
       "      <td>Asian</td>\n",
       "      <td>13153</td>\n",
       "      <td>SLC</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Los Angeles</td>\n",
       "      <td>California</td>\n",
       "      <td>35.0</td>\n",
       "      <td>1958998</td>\n",
       "      <td>2012898</td>\n",
       "      <td>3971896</td>\n",
       "      <td>85417</td>\n",
       "      <td>1485425</td>\n",
       "      <td>2.86</td>\n",
       "      <td>CA</td>\n",
       "      <td>White</td>\n",
       "      <td>2177650</td>\n",
       "      <td>LOS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Tulsa</td>\n",
       "      <td>Oklahoma</td>\n",
       "      <td>35.0</td>\n",
       "      <td>197437</td>\n",
       "      <td>205654</td>\n",
       "      <td>403091</td>\n",
       "      <td>24672</td>\n",
       "      <td>43751</td>\n",
       "      <td>2.37</td>\n",
       "      <td>OK</td>\n",
       "      <td>Black or African-American</td>\n",
       "      <td>72643</td>\n",
       "      <td>TUL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Seattle</td>\n",
       "      <td>Washington</td>\n",
       "      <td>35.5</td>\n",
       "      <td>345659</td>\n",
       "      <td>338784</td>\n",
       "      <td>684443</td>\n",
       "      <td>29364</td>\n",
       "      <td>119840</td>\n",
       "      <td>2.13</td>\n",
       "      <td>WA</td>\n",
       "      <td>Hispanic or Latino</td>\n",
       "      <td>43060</td>\n",
       "      <td>SEA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Camden</td>\n",
       "      <td>New Jersey</td>\n",
       "      <td>27.9</td>\n",
       "      <td>36437</td>\n",
       "      <td>39694</td>\n",
       "      <td>76131</td>\n",
       "      <td>1425</td>\n",
       "      <td>11317</td>\n",
       "      <td>3.0</td>\n",
       "      <td>NJ</td>\n",
       "      <td>White</td>\n",
       "      <td>19907</td>\n",
       "      <td>CNJ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Omaha</td>\n",
       "      <td>Nebraska</td>\n",
       "      <td>34.2</td>\n",
       "      <td>218789</td>\n",
       "      <td>225098</td>\n",
       "      <td>443887</td>\n",
       "      <td>24503</td>\n",
       "      <td>48263</td>\n",
       "      <td>2.47</td>\n",
       "      <td>NE</td>\n",
       "      <td>Hispanic or Latino</td>\n",
       "      <td>63516</td>\n",
       "      <td>OMA</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             city         state median_age male_population female_population  \\\n",
       "0    Philadelphia  Pennsylvania       34.1          741270            826172   \n",
       "1      Fort Myers       Florida       37.3           36850             37165   \n",
       "2          Laredo         Texas       28.8          124305            131484   \n",
       "3       New Haven   Connecticut       29.9           63765             66545   \n",
       "4  Salt Lake City          Utah       32.1           98364             94296   \n",
       "5     Los Angeles    California       35.0         1958998           2012898   \n",
       "6           Tulsa      Oklahoma       35.0          197437            205654   \n",
       "7         Seattle    Washington       35.5          345659            338784   \n",
       "8          Camden    New Jersey       27.9           36437             39694   \n",
       "9           Omaha      Nebraska       34.2          218789            225098   \n",
       "\n",
       "  total_population veteran_population foreign_born_population  \\\n",
       "0          1567442              61995                  205339   \n",
       "1            74015               4312                   15365   \n",
       "2           255789               4921                   68427   \n",
       "3           130310               2567                   25871   \n",
       "4           192660               6829                   32166   \n",
       "5          3971896              85417                 1485425   \n",
       "6           403091              24672                   43751   \n",
       "7           684443              29364                  119840   \n",
       "8            76131               1425                   11317   \n",
       "9           443887              24503                   48263   \n",
       "\n",
       "  avg_household_size state_abbr                               race race_count  \\\n",
       "0               2.61         PA                              Asian     122721   \n",
       "1               2.45         FL                              White      50169   \n",
       "2               3.66         TX  American Indian and Alaska Native       1253   \n",
       "3               2.48         CT  American Indian and Alaska Native       2205   \n",
       "4               2.38         UT                              Asian      13153   \n",
       "5               2.86         CA                              White    2177650   \n",
       "6               2.37         OK          Black or African-American      72643   \n",
       "7               2.13         WA                 Hispanic or Latino      43060   \n",
       "8                3.0         NJ                              White      19907   \n",
       "9               2.47         NE                 Hispanic or Latino      63516   \n",
       "\n",
       "  port_code  \n",
       "0       PHI  \n",
       "1       FMY  \n",
       "2       LAR  \n",
       "3       NWH  \n",
       "4       SLC  \n",
       "5       LOS  \n",
       "6       TUL  \n",
       "7       SEA  \n",
       "8       CNJ  \n",
       "9       OMA  "
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fpath = './us-cities-demographics.csv'\n",
    "df_city_demos = spark.read.format('csv').option('header', 'true').option('delimiter', ';').load(fpath)\n",
    "cleaned_city_demo_data = clean_city_demo_data(df_city_demos)\n",
    "dim_city_demo = cleaned_city_demo_data.select(col('City').alias('city'),\n",
    "                                              col('State').alias('state'),\n",
    "                                              col('Median Age').alias('median_age'),\n",
    "                                              col('Male Population').alias('male_population'),\n",
    "                                              col('Female Population').alias('female_population'),\n",
    "                                              col('Total Population').alias('total_population'),\n",
    "                                              col('Number of Veterans').alias('veteran_population'),\n",
    "                                              col('Foreign-born').alias('foreign_born_population'),\n",
    "                                              col('Average Household Size').alias('avg_household_size'),\n",
    "                                              col('State Code').alias('state_abbr'),\n",
    "                                              col('Race').alias('race'),\n",
    "                                              col('Count').alias('race_count'))\\\n",
    "                                      .withColumn('port_code', city_to_i94port_code(col('city')))\\\n",
    "                                      .dropna(subset=['port_code'])\n",
    "dim_city_demo.toPandas().head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>year</th>\n",
       "      <th>month</th>\n",
       "      <th>origin_country</th>\n",
       "      <th>port_code</th>\n",
       "      <th>arr_date</th>\n",
       "      <th>dep_date</th>\n",
       "      <th>mode</th>\n",
       "      <th>addr_state_abbr</th>\n",
       "      <th>age</th>\n",
       "      <th>reason</th>\n",
       "      <th>gender</th>\n",
       "      <th>temperature_avg</th>\n",
       "      <th>city_median_age</th>\n",
       "      <th>city_total_population</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2016.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>Romania</td>\n",
       "      <td>DAL</td>\n",
       "      <td>2016-04-01</td>\n",
       "      <td>2016-07-06</td>\n",
       "      <td>1.0</td>\n",
       "      <td>TX</td>\n",
       "      <td>63.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>M</td>\n",
       "      <td>16.259</td>\n",
       "      <td>32.6</td>\n",
       "      <td>1300082</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2016.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>India</td>\n",
       "      <td>DAL</td>\n",
       "      <td>2016-04-01</td>\n",
       "      <td>2016-09-02</td>\n",
       "      <td>1.0</td>\n",
       "      <td>TX</td>\n",
       "      <td>47.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>F</td>\n",
       "      <td>16.259</td>\n",
       "      <td>32.6</td>\n",
       "      <td>1300082</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2016.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>Brazil</td>\n",
       "      <td>DAL</td>\n",
       "      <td>2016-04-01</td>\n",
       "      <td>2016-04-07</td>\n",
       "      <td>1.0</td>\n",
       "      <td>CA</td>\n",
       "      <td>42.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>F</td>\n",
       "      <td>16.259</td>\n",
       "      <td>32.6</td>\n",
       "      <td>1300082</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2016.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>Israel</td>\n",
       "      <td>DAL</td>\n",
       "      <td>2016-04-02</td>\n",
       "      <td>2016-04-24</td>\n",
       "      <td>1.0</td>\n",
       "      <td>TX</td>\n",
       "      <td>68.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>M</td>\n",
       "      <td>16.259</td>\n",
       "      <td>32.6</td>\n",
       "      <td>1300082</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2016.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>Australia</td>\n",
       "      <td>DAL</td>\n",
       "      <td>2016-04-02</td>\n",
       "      <td>2016-04-24</td>\n",
       "      <td>1.0</td>\n",
       "      <td>PA</td>\n",
       "      <td>43.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>F</td>\n",
       "      <td>16.259</td>\n",
       "      <td>32.6</td>\n",
       "      <td>1300082</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2016.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>France</td>\n",
       "      <td>DAL</td>\n",
       "      <td>2016-04-03</td>\n",
       "      <td>2016-04-14</td>\n",
       "      <td>1.0</td>\n",
       "      <td>TX</td>\n",
       "      <td>39.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>M</td>\n",
       "      <td>16.259</td>\n",
       "      <td>32.6</td>\n",
       "      <td>1300082</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>2016.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>Italy</td>\n",
       "      <td>DAL</td>\n",
       "      <td>2016-04-03</td>\n",
       "      <td>2016-04-13</td>\n",
       "      <td>1.0</td>\n",
       "      <td>CA</td>\n",
       "      <td>39.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>M</td>\n",
       "      <td>16.259</td>\n",
       "      <td>32.6</td>\n",
       "      <td>1300082</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>2016.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>Indonesia</td>\n",
       "      <td>DAL</td>\n",
       "      <td>2016-04-03</td>\n",
       "      <td>2016-05-16</td>\n",
       "      <td>1.0</td>\n",
       "      <td>MO</td>\n",
       "      <td>22.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>M</td>\n",
       "      <td>16.259</td>\n",
       "      <td>32.6</td>\n",
       "      <td>1300082</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>2016.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>India</td>\n",
       "      <td>DAL</td>\n",
       "      <td>2016-04-03</td>\n",
       "      <td>2016-04-14</td>\n",
       "      <td>1.0</td>\n",
       "      <td>KS</td>\n",
       "      <td>39.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>M</td>\n",
       "      <td>16.259</td>\n",
       "      <td>32.6</td>\n",
       "      <td>1300082</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>2016.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>Thailand</td>\n",
       "      <td>DAL</td>\n",
       "      <td>2016-04-03</td>\n",
       "      <td>2016-04-09</td>\n",
       "      <td>1.0</td>\n",
       "      <td>TX</td>\n",
       "      <td>54.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>F</td>\n",
       "      <td>16.259</td>\n",
       "      <td>32.6</td>\n",
       "      <td>1300082</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     year  month origin_country port_code    arr_date    dep_date  mode  \\\n",
       "0  2016.0    4.0        Romania       DAL  2016-04-01  2016-07-06   1.0   \n",
       "1  2016.0    4.0          India       DAL  2016-04-01  2016-09-02   1.0   \n",
       "2  2016.0    4.0         Brazil       DAL  2016-04-01  2016-04-07   1.0   \n",
       "3  2016.0    4.0         Israel       DAL  2016-04-02  2016-04-24   1.0   \n",
       "4  2016.0    4.0      Australia       DAL  2016-04-02  2016-04-24   1.0   \n",
       "5  2016.0    4.0         France       DAL  2016-04-03  2016-04-14   1.0   \n",
       "6  2016.0    4.0          Italy       DAL  2016-04-03  2016-04-13   1.0   \n",
       "7  2016.0    4.0      Indonesia       DAL  2016-04-03  2016-05-16   1.0   \n",
       "8  2016.0    4.0          India       DAL  2016-04-03  2016-04-14   1.0   \n",
       "9  2016.0    4.0       Thailand       DAL  2016-04-03  2016-04-09   1.0   \n",
       "\n",
       "  addr_state_abbr   age  reason gender temperature_avg city_median_age  \\\n",
       "0              TX  63.0     2.0      M          16.259            32.6   \n",
       "1              TX  47.0     2.0      F          16.259            32.6   \n",
       "2              CA  42.0     1.0      F          16.259            32.6   \n",
       "3              TX  68.0     2.0      M          16.259            32.6   \n",
       "4              PA  43.0     2.0      F          16.259            32.6   \n",
       "5              TX  39.0     1.0      M          16.259            32.6   \n",
       "6              CA  39.0     1.0      M          16.259            32.6   \n",
       "7              MO  22.0     3.0      M          16.259            32.6   \n",
       "8              KS  39.0     1.0      M          16.259            32.6   \n",
       "9              TX  54.0     1.0      F          16.259            32.6   \n",
       "\n",
       "  city_total_population  \n",
       "0               1300082  \n",
       "1               1300082  \n",
       "2               1300082  \n",
       "3               1300082  \n",
       "4               1300082  \n",
       "5               1300082  \n",
       "6               1300082  \n",
       "7               1300082  \n",
       "8               1300082  \n",
       "9               1300082  "
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# join the dimension tables to create the fact table and select the columns for the fact table\n",
    "# we use distinct() to remove duplicate entries which will occur from the denormalized race data\n",
    "# in the city demographic dimension data being joined with the other tables\n",
    "fact_imm = dim_imm.join(dim_temp, (dim_imm.port_code == dim_temp.port_code) & (dim_imm.month == month(dim_temp.date)))\\\n",
    "             .join(dim_city_demo, (dim_imm.port_code == dim_city_demo.port_code))\\\n",
    "             .select('year',\n",
    "                     'month',\n",
    "                     i94cit_code_to_country(col('origin_country_code')).alias('origin_country'),\n",
    "                     dim_imm.port_code,\n",
    "                     'arr_date',\n",
    "                     'dep_date',\n",
    "                     'mode',\n",
    "                     'addr_state_abbr',\n",
    "                     'age',\n",
    "                     'reason',\n",
    "                     'gender',\n",
    "                     col('avg_temperature').alias('temperature_avg'),\n",
    "                     col('median_age').alias('city_median_age'),\n",
    "                     col('total_population').alias('city_total_population')).distinct()\n",
    "fact_imm.toPandas().head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### 4.2 Data Quality Checks\n",
    "\n",
    "We will perform data quality check to ensure our fact and dim tables are not empyt.  We will also check that our renaming of columns has run successfully.  In addition because we did some aggregation on the temperature data to get the most recent 12 months for each city we will checkf that a city has 12 months of avg montly temperatures."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'has_rows' checks passed\n"
     ]
    }
   ],
   "source": [
    "def has_rows(df, name):\n",
    "    \"\"\"\n",
    "    Check if there's at least one row in a dataframe\n",
    "    \n",
    "    Params:\n",
    "    df: spark dataframe\n",
    "    name: friendly printable dataframe name\n",
    "    \"\"\"\n",
    "    if df.count() == 0:\n",
    "        print(f\"'has_rows' validation failed. DataFrame {name} has 0 rows\")\n",
    "        return False\n",
    "    return True\n",
    "\n",
    "dfs = {\n",
    "    'fact_immigrations': fact_imm,\n",
    "    'dim_immigrations': dim_imm,\n",
    "    'dim_temperatures': dim_temp,\n",
    "    'dim_city_demos': dim_city_demo\n",
    "}\n",
    "\n",
    "if not all([has_rows(df, name) for name, df in dfs.items()]):\n",
    "    raise Exception('has_rows checks failed')\n",
    "else:\n",
    "    print(\"'has_rows' checks passed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'has_required_cols' checks passed\n"
     ]
    }
   ],
   "source": [
    "def has_required_cols(df, required_cols, name):\n",
    "    \"\"\"\n",
    "    Check if dataframe has all the required columns\n",
    "    \n",
    "    Params:\n",
    "    df: spark dataframe\n",
    "    required_cols: list with required columns in desired order\n",
    "    name: friendly printable dataframe name\n",
    "    \"\"\"\n",
    "    if len(df.columns) != len(required_cols):\n",
    "        print(f\"'has_required_cols' validation failed. Number of cols do not match for DataFrame {name}\")\n",
    "    if not all([df.columns[i] == required_cols[i] for i in range(len(df.columns))]):\n",
    "        print(f\"'has_required_cols' validation failed.  DataFrame {name} has columns: {df.columns}\"\n",
    "              f\"but expected {required_cols}\")\n",
    "        return False\n",
    "    return True\n",
    "\n",
    "required_cols_per_table = [\n",
    "    {\n",
    "        'name': 'fact_immigrations',\n",
    "        'dataframe': fact_imm,\n",
    "        'required_cols': ['year', 'month', 'origin_country', 'port_code', 'arr_date', 'dep_date', 'mode',\n",
    "                          'addr_state_abbr', 'age', 'reason', 'gender', 'temperature_avg', 'city_median_age',\n",
    "                          'city_total_population']\n",
    "    },\n",
    "    {\n",
    "        'name': 'dim_immigrations',\n",
    "        'dataframe': dim_imm,\n",
    "        'required_cols': ['year', 'month', 'origin_country_code', 'port_code', 'arr_date', 'dep_date', 'mode',\n",
    "                          'addr_state_abbr', 'age', 'reason', 'birth_year', 'gender'],\n",
    "    },\n",
    "    {\n",
    "        'name': 'dim_temperatures',\n",
    "        'dataframe': dim_temp,\n",
    "        'required_cols': ['date', 'avg_temperature', 'city', 'country', 'latitude', 'longitude', 'port_code']\n",
    "    },\n",
    "    {\n",
    "        'name': 'dim_city_demos',\n",
    "        'dataframe': dim_city_demo,\n",
    "        'required_cols': ['city', 'state', 'median_age', 'male_population', 'female_population', 'total_population',\n",
    "                          'veteran_population', 'foreign_born_population', 'avg_household_size', 'state_abbr',\n",
    "                          'race', 'race_count', 'port_code']\n",
    "    }\n",
    "]\n",
    "if not all([has_required_cols(r['dataframe'], r['required_cols'], r['name']) for r in required_cols_per_table]):\n",
    "    raise Exception('has_required_cols checks failed')\n",
    "else:\n",
    "    print(\"'has_required_cols' checks passed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'check_12_temps_per_city' passed\n"
     ]
    }
   ],
   "source": [
    "# Simple sanity check for temperature df manipulations\n",
    "def check_12_temps_per_city(dim_temp, city):\n",
    "    \"\"\" Make sure there are only 12 temps for a city\"\"\"\n",
    "    if dim_temp.select(dim_temp.columns).where(dim_temp.city == city).count() == 12:\n",
    "        return True\n",
    "    return False\n",
    "city = 'San Francisco'\n",
    "if not check_12_temps_per_city(dim_temp, city):\n",
    "    raise Exception(f\"'check_12_temps_per_city' validation failed for city '{city}'\")\n",
    "else:\n",
    "    print(\"'check_12_temps_per_city' passed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### 4.3 Save data for later load\n",
    "Save the data to parquet format for loading into HDFS-based RDBMS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# save for later loads\n",
    "output_data = './output_data/'\n",
    "# can make mode 'append' for dim_imm and fact_imm once deployed in production\n",
    "dim_imm.write.mode('overwrite').partitionBy('year', 'month').parquet(os.path.join(output_data, 'dim_immigrations'))\n",
    "dim_temp.write.mode('overwrite').partitionBy('port_code').parquet(os.path.join(output_data, 'dim_temperatures'))\n",
    "dim_city_demo.write.mode('overwrite').partitionBy('port_code').parquet(os.path.join(output_data, 'dim_city_demographics'))\n",
    "fact_imm.write.mode('overwrite').partitionBy('year', 'month').parquet(os.path.join(output_data, 'fact_immigrations'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### 4.4 Data dictionary \n",
    "The attribute names of the various dataset have been renamed for consistency to the end user\n",
    "\n",
    "##### Facts table:\n",
    "- `year`: 4 digit year of entry\n",
    "- `month`: numeric month of entry\n",
    "- `origin_country`: country of origin\n",
    "- `port_code`: 3 digit code for port of entry\n",
    "- `arr_date`: arrival date in USA (SAS date numeric field, time delta since 1/1/1960)\n",
    "- `dep_date`: departure date from USA (SAS date numeric field, time delta since 1/1/1960)\n",
    "- `mode`: enumeration of entry type (1=air, 2=sea, 3=land, 9='not reported')\n",
    "- `addr_state_abbr`: state of address during stay in the US\n",
    "- `age`: age of respondent\n",
    "- `reason`: enumeration representing reason for visit (1=business, 2=pleasure, 3=student)\n",
    "- `gender`: gender of respondent\n",
    "- `temperature_avg`: average temperature of the port city\n",
    "- `city_median_age`: median age port city\n",
    "- `city_total_population`: total population of port city\n",
    "\n",
    "##### Immigration dimension table:\n",
    "- `year`: 4 digit year of entry\n",
    "- `month`: numeric month of entry\n",
    "- `origin_country_code`: 3 digit code for country of origin\n",
    "- `port_code`: 3 digit code for port of entry\n",
    "- `arr_date`: arrival date in USA (SAS date numeric field, time delta since 1/1/1960)\n",
    "- `dep_date`: departure date from USA (SAS date numeric field, time delta since 1/1/1960)\n",
    "- `mode`: enumeration of entry type (1=air, 2=sea, 3=land, 9='not reported')\n",
    "- `addr_state_abbr`: state of address during stay in the US\n",
    "- `age`: age of respondent\n",
    "- `reason`: enumeration representing reason for visit (1=business, 2=pleasure, 3=student)\n",
    "- `birth_year`: 4 digit birth year of respondent\n",
    "- `gender`: gender of respondent\n",
    "\n",
    "##### Temperature dimension table:\n",
    "- `date`: date of measurement (YYYY-MM-DD)\n",
    "- `avg_temperature`: average temperature recorded\n",
    "- `city`: city of measurement\n",
    "- `country`: country of measurement\n",
    "- `latitude`: geographic latitude\n",
    "- `longitude`: geographic longitude\n",
    "- `port_code`: 3 digit code for port of entry\n",
    "\n",
    "##### City Demographic dimension table:\n",
    "- `city`: city/municipality\n",
    "- `state`: US state in which the city belongs\n",
    "- `median_age`: median age of resident (decimal)\n",
    "- `male_population`: count of male residents\n",
    "- `female_population`: count of female residents\n",
    "- `total_population`: count of all residents\n",
    "- `veteran_population`: count of all veterans\n",
    "- `foreign_born_population`: count of all foreign born residents\n",
    "- `avg_household_size`: average number of residents per household\n",
    "- `state_abbr`: state abbreviation\n",
    "- `race`: race identified by respondent\n",
    "- `race_count`: number of respondents identifying as reported race\n",
    "- `port_code`: 3 digit code for port of entry\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### Step 5: Complete Project Write Up\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "__Clearly state the rationale for the choice of tools and technologies for the project:__\n",
    "\n",
    "*    PySpark was used for the ease of exploring data.  It supports both functional and SQL style queries, and data can easily be parallelized by spreading the dataframes over several workers in a spark cluster"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "Write a description of how you would approach the problem differently under the following scenarios:\n",
    "\n",
    "__The data was increased by 100x:__\n",
    "   \n",
    "* If the data were increased 100x then we would scale up the number of workers and parallelize processing.\n",
    "        \n",
    "__The data populates a dashboard that must be updated on a daily basis by 7am every day:__\n",
    "\n",
    "*    We could run an Airflow DAG to process new data on a schedule and load it to the destination before 7am.  We could use SLAs and alarms to make sure the data is updated in time.\n",
    "        \n",
    "        \n",
    "__The database needed to be accessed by 100+ people:__\n",
    "\n",
    "*    If the resulting data model needs to be accessed by 100+ people we should consider loding the data in Redshift (ie. warehousing it) so that we get the benefits of parallelized queries.\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
